{"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/ahcamachod/1904-word2vec-entrenamiento-de-word-embedding/blob/main/word2vec_entrenamiento.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"Npq9KuJ0XUQQ"},"source":["#Word2Vec: Entrenamiento de Word Embedding\n","\n","En este notebook encontrarás el desarrollo del proyecto para modelar tus propias representaciones de Word Embedding utilizando **Word2Vec** .\n","\n","\n","Los principales recursos de Python que utilizaremos como base para nuestro modelaje se encuentran en:\n","\n","\n","*https://spacy.io/*\n","\n","*https://radimrehurek.com/gensim/models/word2vec.html*\n","\n","\n","La documentación referente al diseño de las arquitecturas Word2Vec la encontramos en:\n","\n","\n","*https://arxiv.org/pdf/1301.3781.pdf*"]},{"cell_type":"markdown","metadata":{"id":"6TougNn-s4c5"},"source":["#**<font color=red>EXPLORANDO SPACY</font>**\n","\n","---\n","\n","\n","\n","Vamos a:\n","\n","1°)Obtener Noticias\n","\n","2°)Realizar la Vectorizacion de los titulares\n","\n","3°)Clasificar a que categorias pertenecen estas noticias\n","\n","Con estos elementos vamos a colocar un Modelo en produccion en una API."]},{"cell_type":"markdown","metadata":{"id":"k00CIiN3audI"},"source":["##**INICIANDO SPACY**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"20SJg0oGzfUZ"},"outputs":[],"source":["# Linea de codigo contruida en el modelo para construir nuestro vocabulario y para ver como lo construyo.\n","\n","import logging\n","\n","logging.basicConfig(format=\"%(asctime)s - %(message)s\", level=logging.INFO)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1W5edpctzsnG"},"outputs":[],"source":["# 1°) Importamos bibliotecas y nuestros datasets, en mi caso lo importo desde la pc y NO del drive --->\n","\n","import pandas as pd\n","\n","noticias_train = pd.read_csv(\"/content/noticias_entrenamiento.csv\")\n","noticias_test = pd.read_csv(\"/content/noticias_prueba.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":68,"status":"ok","timestamp":1721422790505,"user":{"displayName":"mariana ibarra","userId":"08710446199790177814"},"user_tz":180},"id":"K_W6HuGBtuNj","outputId":"88f83c6f-6b61-4c43-e5d5-d625af43a8dc"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(91844, 7)"]},"metadata":{},"execution_count":3}],"source":["# 2°) Vemos la forma de nuestro dataset de Entrenamiento y vemos una muestra con 2 de los registros del dataset --->\n","\n","noticias_train.shape\n","\n","# Nos devuelve la cantidad de filas y columnas."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":199},"executionInfo":{"elapsed":55,"status":"ok","timestamp":1721422790506,"user":{"displayName":"mariana ibarra","userId":"08710446199790177814"},"user_tz":180},"id":"sPcVbPo9tuQL","outputId":"883186b1-325d-4819-901e-2e641e2e74d9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                     fecha                                             titulo  \\\n","49544  2022-03-30 15:03:01  La inflación se desboca en España, impulsada p...   \n","86621  2022-03-22 05:30:00  Cae una mujer de 80 años tras arrancarle la vi...   \n","\n","      pais                                           extracto  \\\n","49544   FR  La inflación bate récords en España: 9,8%, su ...   \n","86621   MX  Autoridades de Japón, arrestaron a una mujer, ...   \n","\n","                                                 resumen  \\\n","49544  España\\n\\nEn España, el aumento de los precios...   \n","86621  Chiba, Japón.- Autoridades de la prefectura de...   \n","\n","                                                  enlace categoria  \n","49544  https://www.rfi.fr/es/europa/20220330-la-infla...  economia  \n","86621  https://www.tribuna.com.mx/mundo/2022/3/21/cae...     mundo  "],"text/html":["\n","  <div id=\"df-406ecae3-71c0-4103-9b6e-54b6999ea615\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>fecha</th>\n","      <th>titulo</th>\n","      <th>pais</th>\n","      <th>extracto</th>\n","      <th>resumen</th>\n","      <th>enlace</th>\n","      <th>categoria</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>49544</th>\n","      <td>2022-03-30 15:03:01</td>\n","      <td>La inflación se desboca en España, impulsada p...</td>\n","      <td>FR</td>\n","      <td>La inflación bate récords en España: 9,8%, su ...</td>\n","      <td>España\\n\\nEn España, el aumento de los precios...</td>\n","      <td>https://www.rfi.fr/es/europa/20220330-la-infla...</td>\n","      <td>economia</td>\n","    </tr>\n","    <tr>\n","      <th>86621</th>\n","      <td>2022-03-22 05:30:00</td>\n","      <td>Cae una mujer de 80 años tras arrancarle la vi...</td>\n","      <td>MX</td>\n","      <td>Autoridades de Japón, arrestaron a una mujer, ...</td>\n","      <td>Chiba, Japón.- Autoridades de la prefectura de...</td>\n","      <td>https://www.tribuna.com.mx/mundo/2022/3/21/cae...</td>\n","      <td>mundo</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-406ecae3-71c0-4103-9b6e-54b6999ea615')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-406ecae3-71c0-4103-9b6e-54b6999ea615 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-406ecae3-71c0-4103-9b6e-54b6999ea615');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-b3ca6e3b-fe34-4ff1-994b-0ab73f48b24d\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b3ca6e3b-fe34-4ff1-994b-0ab73f48b24d')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-b3ca6e3b-fe34-4ff1-994b-0ab73f48b24d button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","repr_error":"0"}},"metadata":{},"execution_count":4}],"source":["noticias_train.sample(2)\n","\n","# Nos devuelve una muestra del dataset con la variable CATEGORIA que es nuestro target, nuestra variable DEPENDIENTE y el TITULAR que es\n","# nuestra variable INDEPENDIENTE el cual vamos a estar procesando para llegar a nuestro Target."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":47,"status":"ok","timestamp":1721422790506,"user":{"displayName":"mariana ibarra","userId":"08710446199790177814"},"user_tz":180},"id":"XGqf7pFktuS5","outputId":"ffd54a57-f9d1-4570-feef-41c2bcf24d87"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(22961, 7)"]},"metadata":{},"execution_count":5}],"source":["# 3°) Hacemos un SHAPE y un SAMPLE con noticias Prueba(test) --->\n","\n","noticias_test.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":181},"executionInfo":{"elapsed":39,"status":"ok","timestamp":1721422790507,"user":{"displayName":"mariana ibarra","userId":"08710446199790177814"},"user_tz":180},"id":"x-DMsg9F07e1","outputId":"09c634e3-be71-44a6-a3fd-7028e252bd00"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                    fecha                                             titulo  \\\n","3349  2022-03-29 20:13:37  Montoya tras debate de vacancia: El pueblo es ...   \n","3170  2022-03-21 21:33:49  Celebra San Miguel de Allende el equinoccio de...   \n","\n","     pais                                           extracto  \\\n","3349   PE  ���Este debate sirvi坦 para desenmascarar a los...   \n","3170   MX  La llegada del equinoccio de primavera se cele...   \n","\n","                                                resumen  \\\n","3349  Jorge Montoya, legislador de Renovaci坦n Popula...   \n","3170  San Miguel de Allende, Guanajuato / 21.03.2022...   \n","\n","                                                 enlace categoria  \n","3349  https://exitosanoticias.pe/v1/montoya-tras-deb...  politica  \n","3170  https://www.milenio.com/politica/comunidad/cel...  politica  "],"text/html":["\n","  <div id=\"df-bb2fd5fb-33ba-43ef-b313-e408ca8f6c7d\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>fecha</th>\n","      <th>titulo</th>\n","      <th>pais</th>\n","      <th>extracto</th>\n","      <th>resumen</th>\n","      <th>enlace</th>\n","      <th>categoria</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>3349</th>\n","      <td>2022-03-29 20:13:37</td>\n","      <td>Montoya tras debate de vacancia: El pueblo es ...</td>\n","      <td>PE</td>\n","      <td>���Este debate sirvi坦 para desenmascarar a los...</td>\n","      <td>Jorge Montoya, legislador de Renovaci坦n Popula...</td>\n","      <td>https://exitosanoticias.pe/v1/montoya-tras-deb...</td>\n","      <td>politica</td>\n","    </tr>\n","    <tr>\n","      <th>3170</th>\n","      <td>2022-03-21 21:33:49</td>\n","      <td>Celebra San Miguel de Allende el equinoccio de...</td>\n","      <td>MX</td>\n","      <td>La llegada del equinoccio de primavera se cele...</td>\n","      <td>San Miguel de Allende, Guanajuato / 21.03.2022...</td>\n","      <td>https://www.milenio.com/politica/comunidad/cel...</td>\n","      <td>politica</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bb2fd5fb-33ba-43ef-b313-e408ca8f6c7d')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-bb2fd5fb-33ba-43ef-b313-e408ca8f6c7d button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-bb2fd5fb-33ba-43ef-b313-e408ca8f6c7d');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-0a8e7e47-58fc-455c-ba67-bdcd648f4288\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0a8e7e47-58fc-455c-ba67-bdcd648f4288')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-0a8e7e47-58fc-455c-ba67-bdcd648f4288 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","repr_error":"0"}},"metadata":{},"execution_count":6}],"source":["noticias_test.sample(2)\n","\n","# Nos devuelve unas noticias diferentes y todas en español."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15700,"status":"ok","timestamp":1721422806173,"user":{"displayName":"mariana ibarra","userId":"08710446199790177814"},"user_tz":180},"id":"2U3e5ztx071E","outputId":"cb3a8cf4-f532-4d5a-a49c-862bfbfbdd4e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting es-core-news-sm==3.7.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.7.0/es_core_news_sm-3.7.0-py3-none-any.whl (12.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from es-core-news-sm==3.7.0) (3.7.5)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.10)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.8)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.9)\n","Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.2.5)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.1.3)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.4.8)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.10)\n","Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.4.1)\n","Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.12.3)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.66.4)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.31.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.8.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.1.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (67.7.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (24.1)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.4.0)\n","Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.25.2)\n","Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.2.0)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.20.1)\n","Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2024.7.4)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.1.5)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.1.7)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (13.7.1)\n","Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.18.1)\n","Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (7.0.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.1.5)\n","Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.2.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.16.1)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.14.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.1.2)\n","Installing collected packages: es-core-news-sm\n","Successfully installed es-core-news-sm-3.7.0\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('es_core_news_sm')\n","\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n","If you are in a Jupyter or Colab notebook, you may need to restart Python in\n","order to load all the package's dependencies. You can do this by selecting the\n","'Restart kernel' or 'Restart runtime' option.\n"]}],"source":["# 4°) Ahora vamos a la pagina de SPACY para saber como INSTALARLA --> USAGE --> LINUX --> X86 --> PIP --> CPU --> SPANISH -->\n","# pip install -U pip setuptools wheel, pip install -U spacy(estos 2 ya vienen instalados en Colab) --> solo instalamos:\n","# python -m spacy download es_core_news_sm, y luego de instalados reiniciamos el ambiente--->\n","\n","!python -m spacy download es_core_news_sm\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8L-iCyaF08Fn"},"outputs":[],"source":["# 5°) Ahora vemos como se hace la importacion del paquete SPACY que fue instalado --> MODELS --> SPANISH --> import spacy --->\n","# --> nlp = spacy.load(\"es_core_news_sm\") ---->\n","\n","import spacy\n","\n","nlp = spacy.load(\"es_core_news_sm\")"]},{"cell_type":"markdown","metadata":{"id":"RGXmMuap2kjE"},"source":["***Defincion:***\n","\n","***Para saber más: Spacy***\n","\n","Spacy es una biblioteca de código abierto gratuita para el procesamiento avanzado de lenguaje natural (PLN o también NLP en inglés).\n","\n","Esta biblioteca fue proyectada específicamente para el uso en producción y ayuda a crear aplicaciones que procesan y “comprenden” grandes volúmenes de textos. Algunos ejemplos de su utilización son para la extracción de información, preprocesamiento y aprendizaje profundo (Deep Learning).\n","\n","Vamos a utilizarla solamente con los recursos que necesitaremos, pero a continuación dejo el link de la documentación de la biblioteca Spacy para quien desee aprender y descubrir más detalles sobre sus aplicaciones."]},{"cell_type":"markdown","metadata":{"id":"nS0BHqw7a0WU"},"source":["##**SPACY Y SUS ESTRUCTURAS**\n","\n","Vamos a continuar entonces aquí investigando un poco más sobre el paquete spacy y sus estructuras. Vamos a la documentación y vengamos a la opción que dice usage o uso. Entonces nosotros ya instalamos spacy. Veamos aquí un poco más donde dice spaCy 101.\n","\n","Vamos a darle clic y esta opción nos dice todo lo que necesitamos saber para poder comenzar a trabajar con spaCy. Si ya has trabajado con procesamiento de lenguaje natural entonces debes de estar familiarizado pero si no, a través de esta documentación vas a poder comprender un poco mejor cómo trabajar con este ecosistema.\n","\n","Entonces nos dice lo que muestra algunos features, dice que hace la tokenización, hace la etiqueta, las partes de la frase entonces le dice por ejemplo si es un verbo, un nombre o adverbio, adjetivo, etcétera. Hacer parsing de dependencia, asignado dependencia sintáctica, etiquetas de dependencia sintáctica describiendo la relación entre tokens individuales como sujeto u objeto, lemmatización, sentence boundary, por ejemplo Named Entity Recognition.\n","\n","Entonces por ejemplo él le da nombres a objetos del mundo real como personas, compañía o lugares. Y si venimos a donde dice linguistic annotations nos da algunos insights dentro de la estructura gramatical de un texto. Entonces si bajamos un poco más y dice luego de que has descargado e instalado una pipeline entrenada puedes cargarlo utilizando spacy.load, que lo que nosotros hicimos en el video anterior.\n","\n","Nosotros importamos spacy y ejecutamos el comando spacy.load, al paquete que descargamos que era una pipeline y lo almacenamos en un objeto llamado nlp. Dice que esto nos devuelve un objeto de tipo lenguaje dice aquí con teniendo todos los componentes y los datos necesarios para procesar el texto.\n","\n","Lo llamamos usualmente nlp. Entonces es este nlp que hay aquí. Cuando llamamos el objeto nlp en un string de texto entonces nos devuelve un doc, un objeto tipo doc procesado. Entonces vean ustedes, aquí dice lo siguiente. Tenemos un ejemplo que nos muestra que en un objeto doc se almacena el objeto nlp al cual se le introduce como parámetro un texto.\n","Y yo puedo iterar sobre este doc porque este doc lo que tiene es una secuencia de tokens. Y cada token tiene un texto, la parte de la frase, par of speech, qué tipo de palabra es, si es verbo, es pronombre, adverbio y la dependencia.\n","\n","***PIPELINES:***\n","\n","Entonces si venimos a la documentación que puedo iterar, para cada token en doc entonces él me puede devolver el texto, en la parte de speech que es, la parte de la frase, o sea qué tipo de palabra es y la dependencia de la misma.\n","\n","Entonces yo puedo ejecutar este comando y él me va a devolver a mí pues todo este ejemplo. Sigamos averiguando un poco más en la documentación donde dice pipelines y aquí nos explica un poco más qué es lo que sucede cuando yo utilizo el objeto nlp sobre un texto.\n","\n","Dice: cuando llamamos a nlp sobre un texto spacy lo primero que hace es tokenizarlo para producir un objeto doc. Este doc es procesado en diversos pasos y esto es lo que se conoce como pipeline de procesamiento. Entonces estos pipelines preentrenados, que fue el que nosotros descargamos, ese core news que descargamos al principio, típicamente incluye un tagger, un lemmatizer, un parser y un reconocedor de entidades.\n","\n","Entonces aquí está lo que es cada uno de ellos. Entonces tokenizers segmenta el texto en tokens. El parser asigna labels de dependencia, etiquetas de dependencia, el ner detecta y etiqueta entidades nombradas que son nombres de personas, gente reconocida y así sucesivamente. Entonces aquí está la gráfica del pipeline que ejecuta el objeto nlp sobre un texto para transformarlo en doc."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sSeCn9oQzu5i"},"outputs":[],"source":["# 1°) Vamos a ver un ejemplo, creamos un texto --->\n","\n","texto = \"Big Data es una ciencia que nos permite trabajar velozmente con grandes volumenes de datos.\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"02FtlAl52iDR"},"outputs":[],"source":["# 2°) Ahora vamos a aplicar NLP al texto y lo vamos a almacenar en una Variable llamada DOC, un objeto DOC que se va a\n","# crear con NLP del texto --->\n","\n","doc = nlp(texto)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":36,"status":"ok","timestamp":1721422814159,"user":{"displayName":"mariana ibarra","userId":"08710446199790177814"},"user_tz":180},"id":"AE4Th6km2iIR","outputId":"b3c65130-320b-49c8-b411-503339658315"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["spacy.tokens.doc.Doc"]},"metadata":{},"execution_count":11}],"source":["# 3°) Si le preguntamos que tipo de objeto es --->\n","\n","type(doc)\n","\n","# Es un objeto tipo SPACY TOKENS DOC, y de esta manera nosotros podemos Iterar, nos puede devolver el texto, la frase, que tipo\n","# de palabra es y la dependencia."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":38,"status":"ok","timestamp":1721422814162,"user":{"displayName":"mariana ibarra","userId":"08710446199790177814"},"user_tz":180},"id":"hNaK0ZYJ2iKt","outputId":"0e1b9092-0efb-45f4-f4ae-7bbec364c30b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Big"]},"metadata":{},"execution_count":12}],"source":["# 4°) Vamos a ver que tiene DOC, si yo puedo Iterar sobre DOC, yo puedo ver  --->\n","\n","doc[0]\n","\n","# Nos devuelve la posicion 0 que es la palabra Big"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":34,"status":"ok","timestamp":1721422814162,"user":{"displayName":"mariana ibarra","userId":"08710446199790177814"},"user_tz":180},"id":"iiLL4x_nAIbi","outputId":"6f50a1ec-53f2-4487-c827-28be4690432b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(Big Data,)"]},"metadata":{},"execution_count":13}],"source":["# Podemos ver ENTIDAD, identificar entidadas nombradas, sabe que la frase tiene que ver con Big Data:\n","\n","doc.ents # Metodo"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32,"status":"ok","timestamp":1721422814163,"user":{"displayName":"mariana ibarra","userId":"08710446199790177814"},"user_tz":180},"id":"zae6MVDdAIf0","outputId":"9f7d6c31-ab96-4c77-e84d-543d431b90d4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":14}],"source":["# Podemos preguntar si la posicion 5 es una STOP WORDS, que se detiene las frase en QUE, entonces nos devuelve TRUE:\n","\n","doc[5].is_stop"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30,"status":"ok","timestamp":1721422814163,"user":{"displayName":"mariana ibarra","userId":"08710446199790177814"},"user_tz":180},"id":"TtZN61YqBNz0","outputId":"e7cda32d-e89a-41a7-a59c-e5adc0395ca7"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{},"execution_count":15}],"source":["# Nos dice FALSE porque NO es una STOPO WORDS porque es un verbo:\n","\n","doc[9].is_stop"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":28,"status":"ok","timestamp":1721422814164,"user":{"displayName":"mariana ibarra","userId":"08710446199790177814"},"user_tz":180},"id":"FNR9QdUzBOmh","outputId":"a1d321bc-a4bd-451e-aaa7-f595766fe72f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'ADJ'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":16}],"source":["# Nos devuelve que tipo de palabra es:\n","\n","doc[9].pos_"]},{"cell_type":"markdown","metadata":{"id":"U7UXiY8-BsZX"},"source":["Mi objeto DOC esta compuesto por varios TOKENS y estos tokens pueden ser la puntuacion, palabra, numero, etc."]},{"cell_type":"markdown","metadata":{"id":"mX1QsYrACi6O"},"source":["***Defenicion:***\n","\n","***Spacy: estructura de datos***\n","\n","Entre las principales ventajas de la biblioteca Spacy está su arquitectura, centralizada en dos estructuras de datos principales: Doc y Vocab.\n","\n","***Sobre la estructura Doc ¿Cuál de las siguientes opciones es correcta?***\n","\n","* Doc es un objeto que posee una secuencia de tokens y todas sus anotaciones.\n","\n"," Doc es un objeto con una secuencia de tokens y anotaciones de acuerdo con los modelos de lenguaje que estés utilizando (“es_core_news_sm”, “en_small”...)."]},{"cell_type":"markdown","metadata":{"id":"FnGG3RMibMjR"},"source":["#**<font color=red>PREPROCESAMIENTO CON SPACY</font>**\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"zyfdQuI8bPW_"},"source":["##**PREPROCESAMIENTO DE DATOS**\n","\n","Vamos a tratar nuestros textos, vamos a crear un FUNCION que nos permita a nosotros identificar a los TOKENS, tomar un STRING separarlos en diversos tokens, quitar las puntuaciones, quitar numeros y ponerlo todo en una LISTA de modo que que podemos trabajar con esta lista e ITERAR, y lo vamos a hacer de forma manual. Tambien debemos colocar todo en MINUSCULA, el texto tiene casi 100 mil titulares y para esto es mejor utilizar un GENERATOR EXPRESSION que es un generador que facilita convertir todas las Mayusculas en Minusculas."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kzaVsd2nzWVX"},"outputs":[],"source":["# 1°) Creamos una Variable que se llame TEXTO para el tratamiento y ahi almacenamos nuestro Generador --->\n","\n","texto_para_tratamiento = (titulo.lower() for titulo in noticias_train.titulo) # Generador que coloca todos los titulos del noticias_train en minuscula."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IEUOEgZ7CYtL"},"outputs":[],"source":["# 2°) Definimos una FUNCION para tratar el texto --->\n","\n","def trata_texto(doc): # Recibe un doc.\n","  token_valido = [] # Creamos Lista Vacia para guardar Token que sean validos.\n","  for token in doc: # Iteramos\n","  # Para que sea valido, NO tiene que tener STOP WORDS como \"que\", \"y\", etc y tiene que ser una palabra(is_alpha):\n","    valido = not token.is_stop and token.is_alpha\n","    if valido: # Si es valido\n","      token_valido.append(token.text) # Si es valido, lo guardamos en la lista vacia.\n","  if len(token_valido) > 2: # La frase tiene que ser de mas de 2 palabras, para que pueda ver el contexto de esa frase.\n","      return \" \".join(token_valido) # Nos retorna un join de token valido."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":27,"status":"ok","timestamp":1721422814166,"user":{"displayName":"mariana ibarra","userId":"08710446199790177814"},"user_tz":180},"id":"8d64rGQUCYv1","outputId":"77106ba0-b8af-4d44-c2f1-038c4a9c0855"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Big Data ciencia permite trabajar velozmente volumenes datos'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":19}],"source":["# 3°) Ejecutamos la funcion trata texto al Doc y utilizamos el ejemplo que hicimos anteriormente --->\n","\n","texto = \"Big Data es una ciencia que nos permite trabajar velozmente con grandes volumenes de datos.\"\n","\n","doc = nlp(texto)\n","trata_texto(doc) # Le aplicamos la funcion.\n","\n","# Nos devuelve un STRING con las palabras pero sin incluir las STOP WORDS como \"que\", \"y\" ni tampoco las puntuaciones."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1721422814166,"user":{"displayName":"mariana ibarra","userId":"08710446199790177814"},"user_tz":180},"id":"lVM9JK2SCYyZ","outputId":"bf5ff3ae-71be-46e2-b739-19d723f13f37"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Big Data ciencia permite trabajar velozmente volumenes datos'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":20}],"source":["# 4°) Comprobamos si la FUNCION realmente trabaja, y para esto agregamos numeros, exclamaciones y signos al texto --->\n","\n","texto = \"Big Data 463762983 es una ciencia! que nos permite $%&%$$$ trabajar velozmente con grandes volumenes de datos.\"\n","\n","doc = nlp(texto)\n","trata_texto(doc) # Le aplicamos la funcion.\n","\n","# Podemos ver que la FUNCION trabaja bien."]},{"cell_type":"markdown","metadata":{"id":"8vuuJFVSbS8J"},"source":["##**OPTIMIZANDO EL TRATAMIENTO DE LOS DATOS**\n","\n","Ya creamos una FUNCION que nos permite tratar nuestro texto para que sean solo palabras, ya tenemos nuestro GENERADOR y ahora vamos lo aplicamos para poner todas las palabras del texto en Minuscula en el objeto DOC."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":162531,"status":"ok","timestamp":1721422976673,"user":{"displayName":"mariana ibarra","userId":"08710446199790177814"},"user_tz":180},"id":"Ph_W4v8dzTAD","outputId":"5eeb1ef2-0dbe-433c-b7a3-fecba8587675"},"outputs":[{"output_type":"stream","name":"stdout","text":["2.710200568040212\n"]}],"source":["# 1°) Para aplicarle la funcion y el generador a todos los titulares vamos a utilizar un LIST COMPREHENSION[], tambien vamos a contabilizar\n","# cuanto se demora este proceso --->\n","\n","from time import time # Importamos time para ver demora\n","\n","t0 = time() # T0= Instancia de time que es en este momento, luego creamo la variable con la list comprehension.\n","\n","texto_tratado = [trata_texto(doc) for doc in nlp.pipe(texto_para_tratamiento, batch_size=1000)] # aplicamos la funcion y generador.\n","\n","tf = time() - t0 # Tiempo actual luego de ejecutar la variable de arriba.\n","print(tf/60) # Para que nos imprima el tiempo que demora en minutos, por eso lo dividimos por 60 seg.\n","\n","# Nos devuelve el tiempo que tarda la ejecuacion de la variable."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1721422976674,"user":{"displayName":"mariana ibarra","userId":"08710446199790177814"},"user_tz":180},"id":"l-WVhk-XKz5H","outputId":"69011044-c280-4cff-a119-3fc5c82b2c10"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                              titulo\n","0  tenso debate senado argentino refinanciamiento...\n","1  triunfo dramático cruz azul duelo campeones atlas\n","2      moderar inflación urgente marcha plan calviño\n","3  llega the batman hbo max mira fecha estreno ex...\n","4  guzmán default fmi implicaba ajuste caída prod..."],"text/html":["\n","  <div id=\"df-e61f9975-ceb3-48ba-989a-a533116630da\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>titulo</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>tenso debate senado argentino refinanciamiento...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>triunfo dramático cruz azul duelo campeones atlas</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>moderar inflación urgente marcha plan calviño</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>llega the batman hbo max mira fecha estreno ex...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>guzmán default fmi implicaba ajuste caída prod...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e61f9975-ceb3-48ba-989a-a533116630da')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-e61f9975-ceb3-48ba-989a-a533116630da button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-e61f9975-ceb3-48ba-989a-a533116630da');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-3f06ce6f-8dfe-4bbc-979d-e6a4a8c0ed33\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3f06ce6f-8dfe-4bbc-979d-e6a4a8c0ed33')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-3f06ce6f-8dfe-4bbc-979d-e6a4a8c0ed33 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"# Nos devuelve los titulos en Minuscula sin las STOP WORDS, sin numeros, sin signos y puntuaciones para todo el data set de Entrenamiento\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"titulo\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"triunfo dram\\u00e1tico cruz azul duelo campeones atlas\",\n          \"guzm\\u00e1n default fmi implicaba ajuste ca\\u00edda producci\\u00f3n empleo\",\n          \"moderar inflaci\\u00f3n urgente marcha plan calvi\\u00f1o\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":22}],"source":["# 2°) Ahora vamos a almacenar nuestros titulos tratados de los titulares en un DataFrame --->\n","\n","titulos_tratados = pd.DataFrame({\"titulo\": texto_tratado}) # la variable contiene el el titulo del dataset con la funcion que contiene un funcion y generador.\n","titulos_tratados.head()\n","\n","# Nos devuelve los titulos en Minuscula sin las STOP WORDS, sin numeros, sin signos y puntuaciones para todo el data set de Entrenamiento."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1721422976675,"user":{"displayName":"mariana ibarra","userId":"08710446199790177814"},"user_tz":180},"id":"e2lF878RK0lV","outputId":"98e36a27-11a0-47a0-c13f-df759ca6f9c2"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["91844"]},"metadata":{},"execution_count":23}],"source":["len(titulos_tratados) # nos devuelve la cantidad de filas de texto tratados del dataset de Entrenamiento."]},{"cell_type":"markdown","metadata":{"id":"fk-xQEIvMU1x"},"source":["\n","***Generator Expression***\n","\n","\n","En el siguiente fragmento de código, empleamos el llamado generator expressions:\n","\n","**texto_para_tratamiento = (titulo.lower() for titulo in noticias_train.titulo)**\n","\n","En este caso, podemos comparar los generator expressions a las list comprehensions, con la diferencia de que los primeros “crean” sus elementos solamente cuando son invocados. Como el generator expressions produce un item a la vez, este puede economizar bastante el uso de memoria.\n","\n","* A tratar los datos para entrenar un modelo Word2Vec;\n","* Cómo usar el objeto doc de Spacy para auxiliar en el preprocesamiento de los datos;\n","* Cómo paralelizar el preprocesamiento con el recurso nlp.pipe() de Spacy.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Mh-HYjArbeMS"},"source":["#**<font color=red>HIPERPARAMETROS DE WORD2VEC</font>**\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"bFVStKr3bgd4"},"source":["##**HIPERPARAMETROS WORD2VEC**\n","\n","Vamos a crear el modelo Word2vec, ya preparamos una funcion para tratar nuestros textos y ya tenemos nuestros titulares listos, ahora comenzamos con nuestro modelaje."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MDtXdrgdy3KJ"},"outputs":[],"source":["# 1°) Importamos Word2Vec y luego creamos el Modelo --->\n","\n","from gensim.models import Word2Vec\n","\n","#modelo_w2v = Word2Vec(sg=0, vector_size=300, window=2)\n","\n","# sg=0 viene CBOW por defecto(0), (1)=SKIP-GRAM, size=Vector con 300 posiciones, WINDOW= Ventana, 2 palabras antes y despues de la palabra que\n","# quiero predecir."]},{"cell_type":"markdown","metadata":{"id":"mcIYsBnqfsgb"},"source":["##**AVANZANDO EN LOS HIPERPARAMETROS**\n","\n","Vamos a ver el siguiente problema, supongamos que yo tengo las palabras : ***Alura - Aluras - Aura - Alure***, si yo al momento de modelar necesito solamente la palabra ***Alura*** y NO necesitamos que se repita el resto de las palabras que son incorrectas, para esto necesitamos establecer un PARAMETRO que me permita a mi establecer un limite minimo de palabras para que tenga en cuenta al momento del procesamiento, este parametro es el que se conoce como ***MIN_COUNTS***"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OfyL7g9Tf7RQ"},"outputs":[],"source":["# 1°) Colocamos ese parametro en el modelo --->\n","\n","modelo_w2v = Word2Vec(sg=0, vector_size=300, window=2, min_count=5, alpha = 0.03, min_alpha=0.0007)\n","\n","# MIN_COUNTS=conteo minimo de una palabra, ALPHA=Para llegar el minimo global para que el Algoritmo generalice bien, MIN_ALPHA=Valor minimo de Alpha\n"]},{"cell_type":"markdown","metadata":{"id":"tgFnE5-CpNpm"},"source":["**Definicion:**\n","\n","***Hiperparámetros***\n","\n","Los hiperparámetros son parámetros que configuran la forma que tu modelo será entrenado, por ello se deben establecer antes de la fase de entrenamiento.\n","\n","***Sobre los hiperparámetros de la implementación gensim del modelo Word2Vec:***\n","\n","* MIN_ALPHA es la tasa de aprendizaje mínima, cuando se aplica al decaimiento de alpha, configuramos min_alpha para garantizar que alpha no sea menor que min_alpha..\n","\n","*\n","El hiperparámetro SG configura la arquitectura de entrenamiento (Skip-Gram o CBOW) del modelo Word2Vec, sg=1 usa la arquitectura Skip-Gram y sg=0 emplea CBOW."]},{"cell_type":"markdown","metadata":{"id":"GyrTRkhqb-kT"},"source":["##**VOCABULARIO Y WORD2VEC**\n","\n","Vamos a construir nuestro VOCABULARIO."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FQnmO5wyqKbc"},"outputs":[],"source":["# 1°) Tomamos el modelo y le aplicamos el metodo BUILD_VOCAB --->\n","\n","# modelo_w2v.build_vocab() # El primer parametro que tenemos que adicionar al metodo BUILD_VOCAB es SENTENCE= a una lista de tokens"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"obDqgX_IqKeH"},"outputs":[],"source":["# 2°) Creamos la LISTA DE TOKENS para el parametro con LIST COMPREHENSIONS para BUILD_VOCAB --->\n","\n","titulos_tratados = titulos_tratados.dropna().drop_duplicates() # (a)\n","\n","lista_lista_tokens = [titulo.split(\" \") for titulo in titulos_tratados.titulo]\n","\n","# Nos da un ERROR en SPLIT, nos dice que tenemos que NO se puede aplicar este atributo porque tenemos registro IS NULL!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gW-I7bfZqKo1"},"outputs":[],"source":["# 3°) Vemos cuanto NULOS tenemos --->\n","\n","#titulos_tratados.isnull().value_counts()\n","\n","# Nos muestra 1119 Nulos, tenemos que borar estos nulos y tambien los duplicados."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1721422978066,"user":{"displayName":"mariana ibarra","userId":"08710446199790177814"},"user_tz":180},"id":"HNjLpva0wgVZ","outputId":"d5d5bf7e-8290-437a-e32d-3a14817d65c3"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["82487"]},"metadata":{},"execution_count":29}],"source":["# 4°) Creamos una variable titulos_tratados y almacenamos la variable titulos_tratados haciendo el DROP en nulos y duplicados --->\n","\n","# (a) titulos_tratados = titulos_tratados.dropna().drop_duplicates() # Colocamos la variable antes de lista TOKENS\n","\n","len(titulos_tratados)\n","# Nos quedaron 82707 titulos sin null y duplicate de los 91844 que teniamos"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0HztxHPkwgYV"},"outputs":[],"source":["# 5°) Ahora construimos nuestro VOCABULARIO con las LISTA TOKENS incluida como parametro de SENTENCES --->\n","\n","modelo_w2v.build_vocab(lista_lista_tokens)\n","\n","# NOS CONSTRUYE EL VOCABULARIO."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"32sdX3Wdwga5"},"outputs":[],"source":["# 6°) Ahora vamos a VISUALIZAR que fue lo que hizo el metodo BUILD_VOCAB del modelo modelo_w2v(cbow), pero esta linea de codigo\n","# va al COMIENZO del notebook , en la primer linea --->\n","\n","#import logging\n","\n","#logging.basicConfig(format=\"%(asctime)s - %(message)s\", level=logging.INFO)\n","\n","# Nos devuelve un LOG en la linea del modelo que construye nuestro vocabulario, con el formato que le asignamos como el dia,\n","# la fecha y el mensaje a la derecha... colecto todas las palabras y mi cuenta...etc.\n"]},{"cell_type":"markdown","metadata":{"id":"dD3NyBI2z08f"},"source":["\n","***Para saber más: Word2Vec***\n","\n","Los modelos de representación de palabras, que de alguna forma capturan el contexto en el cual las palabras son utilizadas, fueron un gran avance para el área de procesamiento de lenguaje natural.\n","\n","Si deseas conocer la parte matemática que se desarrolla tras el modelo Word2Vec, te recomiendo la lectura del paper original de Tomas Mikolov.\n","\n","Además, puedes acceder al Embbeding Project de Tensorflow para explorar de forma visual una representación Word2Vec y comprender aún más sobre esta técnica."]},{"cell_type":"markdown","metadata":{"id":"XqiI55UI0f8c"},"source":["#**<font color=red>ENTRENAMIENTO DE WORD2VEC</font>**\n","\n","---\n","Vamos a Entrenar el modelo WORD2VEC para la arquitectura de CBOW : toma el contexto del titular para predecir la palabra."]},{"cell_type":"markdown","metadata":{"id":"AhruKBUz0kSm"},"source":["##**ENTRENAMIENTO CBOW**"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1721422978758,"user":{"displayName":"mariana ibarra","userId":"08710446199790177814"},"user_tz":180},"id":"CjYUH_O_06m0","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6c2c4d69-68d1-4965-8bb3-f3b162d424bb"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['__class__',\n"," '__delattr__',\n"," '__dict__',\n"," '__dir__',\n"," '__doc__',\n"," '__eq__',\n"," '__format__',\n"," '__ge__',\n"," '__getattribute__',\n"," '__gt__',\n"," '__hash__',\n"," '__init__',\n"," '__init_subclass__',\n"," '__le__',\n"," '__lt__',\n"," '__module__',\n"," '__ne__',\n"," '__new__',\n"," '__reduce__',\n"," '__reduce_ex__',\n"," '__repr__',\n"," '__setattr__',\n"," '__sizeof__',\n"," '__str__',\n"," '__subclasshook__',\n"," '__weakref__',\n"," '_adapt_by_suffix',\n"," '_check_corpus_sanity',\n"," '_check_training_sanity',\n"," '_clear_post_train',\n"," '_do_train_epoch',\n"," '_do_train_job',\n"," '_get_next_alpha',\n"," '_get_thread_working_mem',\n"," '_job_producer',\n"," '_load_specials',\n"," '_log_epoch_end',\n"," '_log_epoch_progress',\n"," '_log_progress',\n"," '_log_train_end',\n"," '_raw_word_count',\n"," '_save_specials',\n"," '_scan_vocab',\n"," '_smart_save',\n"," '_train_epoch',\n"," '_train_epoch_corpusfile',\n"," '_worker_loop',\n"," '_worker_loop_corpusfile',\n"," 'add_lifecycle_event',\n"," 'add_null_word',\n"," 'alpha',\n"," 'batch_words',\n"," 'build_vocab',\n"," 'build_vocab_from_freq',\n"," 'cbow_mean',\n"," 'comment',\n"," 'compute_loss',\n"," 'corpus_count',\n"," 'corpus_total_words',\n"," 'create_binary_tree',\n"," 'cum_table',\n"," 'effective_min_count',\n"," 'epochs',\n"," 'estimate_memory',\n"," 'get_latest_training_loss',\n"," 'hashfxn',\n"," 'hs',\n"," 'init_sims',\n"," 'init_weights',\n"," 'layer1_size',\n"," 'lifecycle_events',\n"," 'load',\n"," 'make_cum_table',\n"," 'max_final_vocab',\n"," 'max_vocab_size',\n"," 'min_alpha',\n"," 'min_alpha_yet_reached',\n"," 'min_count',\n"," 'negative',\n"," 'ns_exponent',\n"," 'null_word',\n"," 'predict_output_word',\n"," 'prepare_vocab',\n"," 'prepare_weights',\n"," 'random',\n"," 'raw_vocab',\n"," 'reset_from',\n"," 'running_training_loss',\n"," 'sample',\n"," 'save',\n"," 'scan_vocab',\n"," 'score',\n"," 'seed',\n"," 'seeded_vector',\n"," 'sg',\n"," 'shrink_windows',\n"," 'sorted_vocab',\n"," 'syn1neg',\n"," 'total_train_time',\n"," 'train',\n"," 'train_count',\n"," 'update_weights',\n"," 'vector_size',\n"," 'window',\n"," 'workers',\n"," 'wv']"]},"metadata":{},"execution_count":32}],"source":["# 1°) Primero vamos a dar un DIR al modelo para saber cuantos Features o Atributos tiene --->\n","\n","dir(modelo_w2v)\n","\n","# Nos devuelve un monton de Atributos pero el que nos interesa es el TRAIN porque debemos entrenar nuestro modelo, tambien\n","# tenemos otro parametro importante que es CORPUS_COUNT."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":38518,"status":"ok","timestamp":1721423017273,"user":{"displayName":"mariana ibarra","userId":"08710446199790177814"},"user_tz":180},"id":"AL5DaqLR06pm","colab":{"base_uri":"https://localhost:8080/"},"outputId":"21cd09e5-24f3-4f98-d437-2b7878d0d4bf"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(14593202, 16598820)"]},"metadata":{},"execution_count":33}],"source":["# 2°) Vamos a Entrenar el modelo y recibe como parametro LISTA_LISTA_TOKENS, el Corpus_File, TOTAL_EXAMPLES= la cuenta de la cantidad\n","# de las frases que va a ser igual al CORPUS_COUNT y las EPOCHS= que son el numero de Iteraciones sobre el Corpus --->\n","\n","modelo_w2v.train(lista_lista_tokens, total_examples=modelo_w2v.corpus_count, epochs=30)\n","\n","# Entreno 16598820 palabras crudas y palabras efectivas."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":39,"status":"ok","timestamp":1721423017273,"user":{"displayName":"mariana ibarra","userId":"08710446199790177814"},"user_tz":180},"id":"-UDl4pGY06ud","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2ceed5e2-5aa3-430d-bd2a-aa38867fc742"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('maps', 0.6684199571609497),\n"," ('chrome', 0.5521437525749207),\n"," ('comandos', 0.5513606667518616),\n"," ('android', 0.5427801609039307),\n"," ('emojis', 0.529778242111206),\n"," ('store', 0.5147353410720825),\n"," ('youtube', 0.5113668441772461),\n"," ('zoom', 0.5095242261886597),\n"," ('apple', 0.5088775753974915),\n"," ('apps', 0.5019453763961792)]"]},"metadata":{},"execution_count":34}],"source":["# 3°) Vamos a ver como generalizo nuestro entrenamiento del modelo --->\n","\n","modelo_w2v.wv.most_similar(\"google\") # wv=word vector, le preguntamos palabra semejante a google\n","\n","# Nos devuelve la palabra y el % de confianza de la palabra similar.\n","# Estamos trabajando con CBOW el toma el contexto para descubrir la palabra.\n"]},{"cell_type":"code","source":["# 4°) Vamos a ver como generalizo nuestro entrenamiento del modelo --->\n","\n","modelo_w2v.wv.most_similar(\"microsoft\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H9Doi-gYsFYc","executionInfo":{"status":"ok","timestamp":1721423017274,"user_tz":180,"elapsed":38,"user":{"displayName":"mariana ibarra","userId":"08710446199790177814"}},"outputId":"2af5dd7d-93b1-4c00-b86d-0e097b90e247"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('intel', 0.704036295413971),\n"," ('nvidia', 0.6934400796890259),\n"," ('nothing', 0.691305935382843),\n"," ('ubisoft', 0.6676716208457947),\n"," ('macbook', 0.6125083565711975),\n"," ('inc', 0.6100292205810547),\n"," ('valve', 0.601817786693573),\n"," ('legends', 0.6007205843925476),\n"," ('portátiles', 0.598865807056427),\n"," ('dyson', 0.5903044939041138)]"]},"metadata":{},"execution_count":35}]},{"cell_type":"code","source":["# 5°) Vamos a ver como generalizo nuestro entrenamiento del modelo con respecto al deporte --->\n","\n","modelo_w2v.wv.most_similar(\"barcelona\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hfksavlosFwo","executionInfo":{"status":"ok","timestamp":1721423017274,"user_tz":180,"elapsed":34,"user":{"displayName":"mariana ibarra","userId":"08710446199790177814"}},"outputId":"0eedda98-0680-4d0e-9039-b300ce8aeb5a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('guaireña', 0.5547839999198914),\n"," ('mazatlán', 0.5319907069206238),\n"," ('barça', 0.5109214782714844),\n"," ('fc', 0.4946800172328949),\n"," ('ousmane', 0.48212990164756775),\n"," ('jude', 0.47585150599479675),\n"," ('saltillo', 0.4523231089115143),\n"," ('laporta', 0.4457821547985077),\n"," ('tapatío', 0.4321860074996948),\n"," ('erling', 0.4263615608215332)]"]},"metadata":{},"execution_count":36}]},{"cell_type":"code","source":["# 6°) Vamos a ver como generalizo nuestro entrenamiento del modelo con respecto al deporte --->\n","\n","modelo_w2v.wv.most_similar(\"messi\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ITYds-I2sF52","executionInfo":{"status":"ok","timestamp":1721423017274,"user_tz":180,"elapsed":31,"user":{"displayName":"mariana ibarra","userId":"08710446199790177814"}},"outputId":"49c17768-711e-4fcb-f0ae-4701ad337b49"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('lionel', 0.7293429970741272),\n"," ('scaloni', 0.6510369777679443),\n"," ('roccuzzo', 0.6449397206306458),\n"," ('antonela', 0.6356933116912842),\n"," ('stegen', 0.6066796183586121),\n"," ('neymar', 0.5930041670799255),\n"," ('leo', 0.5732771754264832),\n"," ('silbidos', 0.5675395727157593),\n"," ('psg', 0.5526174306869507),\n"," ('lewandowski', 0.5500613451004028)]"]},"metadata":{},"execution_count":37}]},{"cell_type":"code","source":["# 6°) Vamos a ver como generalizo nuestro entrenamiento del modelo con respecto a auto --->\n","\n","modelo_w2v.wv.most_similar(\"ferrari\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e5modmyWsbX9","executionInfo":{"status":"ok","timestamp":1721423017274,"user_tz":180,"elapsed":27,"user":{"displayName":"mariana ibarra","userId":"08710446199790177814"}},"outputId":"dce6a3b5-377f-4474-a848-429aaf845106"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('leclerc', 0.7700124979019165),\n"," ('bahrein', 0.7064856886863708),\n"," ('verstappen', 0.6950523257255554),\n"," ('sainz', 0.6646941304206848),\n"," ('baréin', 0.6343582272529602),\n"," ('position', 0.6059914231300354),\n"," ('pole', 0.5921818614006042),\n"," ('yeda', 0.570076048374176),\n"," ('barguil', 0.5683915615081787),\n"," ('arabia', 0.5592194199562073)]"]},"metadata":{},"execution_count":38}]},{"cell_type":"markdown","source":["* Nuestro modelo aprendio a travez de la arquitectura de CBOW con un corpus no muy grande de 9 mil palabras y generalizo bastante bien entrenando solo con los titulares de las noticias.\n","\n"," De esta manera Entrenamos el modelo con la arquitectura de CBOW que toma el contexto para predecir la palabra."],"metadata":{"id":"zqnuSPsDsjyJ"}},{"cell_type":"markdown","metadata":{"id":"k_c0Ib1acCe7"},"source":["##**ENTRENAMIENTO SKIP-GRAM**\n","\n","Ahora vamos a Entrenar el modelo con la Arquitectura de SKIP-GRAM."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":93180,"status":"ok","timestamp":1721423110430,"user":{"displayName":"mariana ibarra","userId":"08710446199790177814"},"user_tz":180},"id":"PnvD_HuRylZ5","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9173c23a-2a48-4c94-9fd5-e8e89f2af71f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(14593075, 16598820)"]},"metadata":{},"execution_count":39}],"source":["# 1°) Copiamos codigos e instanciamos el modelo pero con SKIP-GRAM --->\n","\n","# SG=1, WINDOWS=5 ventanas mas grande porque estamos tomando la palabra para predecir el contexto:\n","modelo_w2v_sg = Word2Vec(sg=1, vector_size=300, window=5, min_count=5, alpha = 0.03, min_alpha=0.0007) # Modelo con Parametros\n","\n","modelo_w2v_sg.build_vocab(lista_lista_tokens, progress_per=5000) # Construccion del vocabulario del modelo\n","\n","modelo_w2v_sg.train(lista_lista_tokens, total_examples=modelo_w2v_sg.corpus_count, epochs=30) # Entrenamiento del modelo"]},{"cell_type":"code","source":["# 2°) Ahora vamos a ver como generalizo SKIP-GRAM, copiamos las mismas consultas que hicimos con CBOW y comparamos las\n","# respuestas de las 2 arquietecturas del modelo --->\n","\n","# CBOW:\n","modelo_w2v.wv.most_similar(\"ferrari\")\n","\n","# Nos devuelve LECLARC con 0.75 % ..."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ElGEZry4ttnQ","executionInfo":{"status":"ok","timestamp":1721423110430,"user_tz":180,"elapsed":17,"user":{"displayName":"mariana ibarra","userId":"08710446199790177814"}},"outputId":"7d5a3e73-d384-4edb-901a-0391a60a27e7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('leclerc', 0.7700124979019165),\n"," ('bahrein', 0.7064856886863708),\n"," ('verstappen', 0.6950523257255554),\n"," ('sainz', 0.6646941304206848),\n"," ('baréin', 0.6343582272529602),\n"," ('position', 0.6059914231300354),\n"," ('pole', 0.5921818614006042),\n"," ('yeda', 0.570076048374176),\n"," ('barguil', 0.5683915615081787),\n"," ('arabia', 0.5592194199562073)]"]},"metadata":{},"execution_count":40}]},{"cell_type":"code","source":["# SKIP_GRAM:\n","modelo_w2v_sg.wv.most_similar(\"ferrari\")\n","\n","# Nos devuelve SAINZ con 0.69 % ..."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dlykAuAvttuv","executionInfo":{"status":"ok","timestamp":1721423110430,"user_tz":180,"elapsed":14,"user":{"displayName":"mariana ibarra","userId":"08710446199790177814"}},"outputId":"68015563-aad2-4623-d252-258c19f697dc"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('sainz', 0.7000979781150818),\n"," ('leclerc', 0.6752962470054626),\n"," ('bahrein', 0.6399394273757935),\n"," ('position', 0.6146969795227051),\n"," ('baréin', 0.6063476204872131),\n"," ('bull', 0.5910671949386597),\n"," ('verstappen', 0.5853387117385864),\n"," ('yeda', 0.5136747360229492),\n"," ('charles', 0.5112325549125671),\n"," ('ocon', 0.5073619484901428)]"]},"metadata":{},"execution_count":41}]},{"cell_type":"code","source":["# CBOW:\n","modelo_w2v.wv.most_similar(\"google\")\n","\n","# Nos devuelve MAPS con 0.65 % ..."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MvB74Np43qoU","executionInfo":{"status":"ok","timestamp":1721423110431,"user_tz":180,"elapsed":12,"user":{"displayName":"mariana ibarra","userId":"08710446199790177814"}},"outputId":"410159ab-5f6f-44da-b21f-3c840e7b77ba"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('maps', 0.6684199571609497),\n"," ('chrome', 0.5521437525749207),\n"," ('comandos', 0.5513606667518616),\n"," ('android', 0.5427801609039307),\n"," ('emojis', 0.529778242111206),\n"," ('store', 0.5147353410720825),\n"," ('youtube', 0.5113668441772461),\n"," ('zoom', 0.5095242261886597),\n"," ('apple', 0.5088775753974915),\n"," ('apps', 0.5019453763961792)]"]},"metadata":{},"execution_count":42}]},{"cell_type":"code","source":["# SKIP-GRAM:\n","modelo_w2v_sg.wv.most_similar(\"google\")\n","\n","# Nos devuelve MAPS con 0.70 %...\n","# Nos devuelve un contexto un poco mejor de palabras."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0XySUCCO3x-u","executionInfo":{"status":"ok","timestamp":1721423110431,"user_tz":180,"elapsed":9,"user":{"displayName":"mariana ibarra","userId":"08710446199790177814"}},"outputId":"45980465-5bff-429d-a942-32f189252cca"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('maps', 0.7099429368972778),\n"," ('doodle', 0.5738242268562317),\n"," ('chrome', 0.5727046132087708),\n"," ('caffarena', 0.5116414427757263),\n"," ('gmail', 0.5052133202552795),\n"," ('chromebook', 0.4589812755584717),\n"," ('búsquedas', 0.45614492893218994),\n"," ('desenfocar', 0.4531976282596588),\n"," ('roku', 0.4468194544315338),\n"," ('apk', 0.44516003131866455)]"]},"metadata":{},"execution_count":43}]},{"cell_type":"markdown","source":["* **CBOW :** Tomo el contexto para descubrir la palabra.\n","* **SKIP-GRAM :** Tomo la palabra para descubrir el contexto.\n","\n","**Cual Usar?? :** Entonces hay varias ventajas y desventajas. Ellas se encuentran en el documento de Tomas Mikolov.\n","\n","Aquí él dice por ejemplo que la ventaja utilizando CBOW es que es más rápido y Skip Gram pues se demora más pero Skip Gram puede que generalice mejor tomando un contexto más grande. Sin embargo cada planteamiento dependiendo del proyecto puede ser mejor o más útil o más efectivo que el otro."],"metadata":{"id":"ThlNztOz4eh_"}},{"cell_type":"code","source":["# 3°) Ahora vamos a tomar los 2 modelos y los vamos a almacenar en un archivo de texto para posteriormente poder\n","# cargar nuestro modelo --->\n","\n","# CBOW:\n","modelo_w2v.wv.save_word2vec_format(\"modelo_cbow_300.txt\", binary=False)\n","\n","# SKIP-GRAM:\n","modelo_w2v_sg.wv.save_word2vec_format(\"modelo_sg_300.txt\", binary=False)\n","\n","# Nos aparace cuando lo guardamos en colab."],"metadata":{"id":"g9oCvvNY5U9i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["***Word2Vec: flujo de entrenamiento***\n","\n","Word2Vec.\n","\n","Los hiperparámetros son parámetros que configuran la forma como el modelo será entrenado, por ello deben ser informados antes de la fase de entrenamiento.\n","\n","***Sobre los hiperparámetros de la implementación gensim del modelo Word2Vec:***\n","\n","* Configuración del modelo, construcción del vocabulario y entrenamiento.\n","\n","\n","Primero, configuramos el modelo. Después, construimos el vocabulario a partir del corpus y, finalmente, entrenamos la representación Word2Vec."],"metadata":{"id":"nO1JJSrA6lRH"}},{"cell_type":"markdown","metadata":{"id":"rMvPNYUHcHdE"},"source":["#**<font color=red>VECTORIZANDO DATOS DE ENTRENAMIENTO Y PRUEBA</font>**\n","\n","---\n","\n","Estamos iniciando con el Clasificador de Noticias que toma los Titulares y nos entrega como OUTPUT la categoria a la cual corresponde la noticia.\n","\n"]},{"cell_type":"markdown","source":["##**INICIANDO EL CLASIFICADOR**\n","\n","Nosotros desarrollamos una Funcion TRATA TEXTO que lo que hace es tomar un String, la convertimos en un objeto DOC y a este objeto DOC le aplicamos diversos operaciones para poder quitarle la Puntuacion, lo que NO es palabras y quitar cualquier tipo de Simbolo como asi tambien las Stop Words."],"metadata":{"id":"4ptwuwVx7tbV"}},{"cell_type":"code","source":["# 1°) Copiamos la Funcion TRATA_TEXTO, como NO vamos a trabajar con NLTK ni con STRING, vamos a trabajar con la biblioteca SPACY,\n","# la cargamos y luego pondemos algunas parametros de cosas que NO queremos que utilice --->\n","\n","import spacy\n","\n","nlp = spacy.load(\"es_core_news_sm\", disable=[\"parser\", \"tagger\", \"ner\", \"textcat\"])\n","\n","# Funcion:\n","def tokenizador(texto): # Cambiamos nombre de la Funcion y ahora recibe texto.\n","  doc = nlp(texto) # Creamos el objeto DOC que recibe nlp texto.\n","  token_valido = [] # Creamos Lista Vacia para guardar Token que sean validos.\n","  for token in doc: # Iteramos\n","  # Para que sea valido, NO tiene que tener STOP WORDS como \"que\", \"y\", etc y tiene que ser una palabra(is_alpha):\n","    valido = not token.is_stop and token.is_alpha\n","    if valido: # Si es valido\n","      token_valido.append(token.text.lower()) # Si es valido, lo guardamos en la lista vacia.\n","  return token_valido\n","\n","\n","# Ya tenemos nuestro TOKENIZADOR."],"metadata":{"id":"a8hYWhmz7lKR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 2°) Vamos a tomar un STRING de un TITULAR para probar la funion --->\n","\n","texto = noticias_train.titulo.loc[41958]\n","tokens = tokenizador(texto) # pasamos la funcion tokenizador al texto.\n","print(tokens)\n","\n","# La funcion trabaja bien y nos devuelve los TOKENS limpios."],"metadata":{"id":"CbMjG7qF7lUf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1721423118459,"user_tz":180,"elapsed":9,"user":{"displayName":"mariana ibarra","userId":"08710446199790177814"}},"outputId":"a2e7f308-07ad-44ad-a427-7ce0bcad3758"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['frutas', 'ayudan', 'bajar', 'peso']\n"]}]},{"cell_type":"markdown","metadata":{"id":"LwHxPZgjcRHg"},"source":["##**COMBINACION DE VECTORES**\n","\n","Vamos a realizar la combinacion de cada una de las palabras para generar un VECTOR que represente a cada uno de los TITULARES de noticias, pero para esto tenemos que importar mis modelos de KEYEDVECTORS que almacenamos."]},{"cell_type":"code","source":["# 1°) Importamos KeyedVectors, Numpy(para las matrices) y luego creamos nuestros Modelos --->\n","\n","from gensim.models import KeyedVectors\n","import numpy as np\n","\n","modelo_cbow = KeyedVectors.load_word2vec_format(\"/content/modelo_cbow_300.txt\") # load_word2vec_format= para cargar texto.\n","modelo_sg = KeyedVectors.load_word2vec_format(\"/content/modelo_sg_300.txt\")"],"metadata":{"id":"9J1SQs6S0Ie_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 2°) Ahora creamos la Funcion COMBIANCION DE VECTORES POR SUMA, que recibe los Tokens y el Modelo que vamos a usar --->\n","\n","def combinacion_vectores_por_suma(tokens, modelo):\n","  vector_resultante = np.zeros((1,300)) # Vector Numpy con una matriz de 1,300.\n","  for token in tokens: # Para cada uno de los token en Tokens\n","    try: # Intenta\n","      vector_resultante += modelo.get_vector(token)\n","    except KeyError:\n","      continue\n","  return vector_resultante"],"metadata":{"id":"OAqaISZ40Ihw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 3°) Probamos la Funcion ---->\n","\n","vector_texto = combinacion_vectores_por_suma(tokens, modelo_cbow) # Aplicamos la Funcion al texto que veniamos utilizando[41958]\n","print(vector_texto.shape) # Tamaño.\n","print(vector_texto) # Representacion Vectorial\n","\n","# Nos devuelve el tamaño y representacion vectorial del titular 41958(vector del titular)."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fd0A6OUm0IkH","executionInfo":{"status":"ok","timestamp":1721423127954,"user_tz":180,"elapsed":39,"user":{"displayName":"mariana ibarra","userId":"08710446199790177814"}},"outputId":"4486167d-36f8-4680-ae0e-8ec17c3b0423"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(1, 300)\n","[[-0.1783093   0.4551409   1.20113603  0.23892139 -0.28846563  0.50915633\n","   0.8994507  -0.84211266  0.96918501  0.65691405 -1.65884745 -0.6130172\n","  -1.77031472  2.9145195   0.03093125 -0.31444959 -0.2979071   0.75660521\n","  -1.12727798 -0.38963054 -0.20067446 -2.05806889 -0.26897394 -0.89388443\n","   0.48939249 -0.5873139  -1.49229313  0.04147832  1.79037429  0.20498748\n","   0.33093708  0.38423674  1.29645954 -0.16475333 -0.0718601  -2.04550698\n","  -0.68330836 -1.0583688   0.26943581 -1.30475834  0.22440809 -0.5409074\n","  -0.15102907 -0.30915844  0.47543196  0.36488396 -1.84529944 -1.55044336\n","   0.0689904   1.36901489 -0.24421154 -0.71069192 -0.97837563  0.06932724\n","  -1.0329171  -1.04462859  1.43282366  0.67525192 -0.01121043  0.08296661\n","   1.87076334  1.49012609 -0.54295328  1.70105469 -0.57420567  0.65926257\n","  -0.4942462   2.3551878  -1.3041252   0.83211318  0.07754895  2.15501651\n","   3.49465731  1.55264395  1.34678543 -0.74081644  0.10175785  1.98147202\n","  -0.10415301  0.51462108  2.61001506  1.30171825 -0.51065763 -0.89084379\n","  -1.58720985  0.31356632  2.18348013 -1.65269845 -0.67678743  1.20086727\n","  -0.25644029 -0.07864855 -2.66050263 -2.55202719  0.05968689 -0.48373735\n","   1.26759638 -1.51949553 -0.01693024  0.95127761  1.34423862 -2.20170355\n","   0.45232624  1.28654724 -0.63967073 -0.43741014 -0.71740688 -0.18967408\n","  -1.29559733 -1.89105451 -1.10260964  0.72777012 -0.69283441  0.51124185\n","   0.14792821  0.78922132  0.30938627  1.68798983  1.18851492 -0.47066894\n","  -0.64353685  2.49853598  2.09063013  1.15186741  0.48948964 -0.58255385\n","  -0.90852153 -1.26071543  0.71329684  1.11138232 -2.37371004 -0.38974724\n","  -2.05278495 -2.71916293  0.11389952 -0.2854793  -1.43202914 -0.95718123\n","  -1.37040367  0.17017452  0.49720541  1.00986824  1.20165272  1.54588573\n","   1.3836755  -0.15296544 -2.50466874 -0.54175547  0.23101174 -0.53951565\n","   0.84966138 -0.45741501  0.65712356  0.49156874  1.01123039 -0.41541517\n","  -2.55090057 -0.92909414 -0.46630979  0.91972729 -0.36613402 -0.70078108\n","  -1.94358326  2.18023646  2.55773062 -0.77413999 -3.798774    1.04714\n","  -0.05902083 -1.37543799 -2.48758739 -0.23027399  1.38038844 -0.81285615\n","  -0.03591448  0.00973031 -1.10512073  0.12587252 -0.17143422 -1.31967193\n","  -1.26679218 -0.94076177  0.24977613 -3.96723339  0.40377913  0.52783254\n","  -0.40101934  0.40967727 -0.5774779  -2.84821881  0.28426982 -0.35178889\n","  -0.62523163  1.45612082  0.70819308 -0.46881345  0.01495264  2.06925417\n","   0.0135154  -0.70424475 -0.6134572  -0.09192803  0.25386642 -2.32686511\n","   0.31830969  0.92452871 -0.6993134  -0.82428459 -0.48852705  0.55739004\n","   0.94260722 -0.67220606  0.34848916 -0.94712696  1.76269743 -1.65411942\n","  -0.61205348 -2.2317992  -0.17462346  1.18367964  1.20103804  1.89604428\n","  -0.29966794  0.74040513  0.96019322  0.46808373 -1.84211806 -1.35897588\n","  -1.03085335 -1.05114333  2.0188134   0.86429004 -1.02998187  2.07775488\n","  -0.42535604 -0.81586645  0.06048821  0.79402127  0.51166187 -1.42706711\n","   0.61666565 -1.68105275  1.3960062  -0.89923146 -0.37825603 -1.78483291\n","   1.01128617  0.37257058  0.20273978  0.50641605 -0.7474617   0.94285661\n","   0.43844821  1.85787368  1.71006058 -1.49502408  2.14051586  2.97942922\n","   2.12535861 -1.31028062 -2.17425908  0.70920014  0.01004654  1.62158551\n","  -3.09990501  1.91874453 -0.50787301 -1.06188586  1.7859841  -1.17549674\n","   0.10550278  1.38020868  0.46617262  1.02115612 -0.48271711 -1.62578604\n","   0.88063736  1.52558026 -1.60845947 -0.67571951  0.54644236 -1.92602235\n","   1.60535917  2.43316758 -0.49345356 -0.29741651  0.55201829  0.26968427\n","  -0.83157954  0.81290901 -1.86122686 -1.23473528 -0.3429658   2.29386262\n","   0.04689695 -0.31414226 -0.1456801  -0.83224886 -0.12335207  0.09526643]]\n"]}]},{"cell_type":"markdown","source":["**Definicion:**\n","\n","***El Tokenizador***\n","\n","Hemos explorado y desarrollado funciones auxiliares para utilizar en la creación del clasificador. Entre las funciones desarrolladas, tenemos:\n","\n","* La responsable por el tratamiento y la tokenización de los textos\n","\n","* La que combina los vectores de cada palabra y crea una representación vectorial para toda la frase\n","\n","*La función que crea la representación vectorial de toda la base de datos.\n","\n","***Si una de las funciones recibe como entrada:***\n","\n","\"¡Río de Janeiro 1231231 ***** @#$ es una ciudad maravillosa!\"\n","\n","***Y entrega como salida la siguiente lista:***\n","\n","['río', 'janeiro', 'ciudad', 'maravillosa']\n","\n","***Nota: considera los modelos nlp entrenados en Español:***\n","\n","nlp = spacy.load(\"es_core_news_sm\", disable=[\"parser\", \"ner\", \"tagger\", \"textcat\"])\n","\n","***Se trata de la función:***\n","\n","\n","    def tokenizador(texto):\n","\n","    doc = nlp(texto)\n","   \n","    tokens_validos = []\n","   \n","    for token in doc:\n","       \n","      valido = not token.is_stop and token.is_alpha\n","       \n","     if valido:\n","           \n","         tokens_validos.append(token.text.lower())\n","\n","    return  tokens_validos\n","\n","**Aquí estamos removiendo los stop words y caractéres no alfabéticos, y dejando palabras en minúscula, retornando exactamente lo que fue solicitado.**   "],"metadata":{"id":"qBh2HxBL3XNF"}},{"cell_type":"markdown","metadata":{"id":"1uo1Ui1vcTm3"},"source":["##**VECTORIZANDO LOS TITULARES**\n","\n","Vamos a Vectorizar todos los textos."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":885394,"status":"ok","timestamp":1721428954336,"user":{"displayName":"mariana ibarra","userId":"08710446199790177814"},"user_tz":180},"id":"Ch5rHrxuyQii","colab":{"base_uri":"https://localhost:8080/"},"outputId":"981307b1-77eb-4297-c4b0-d9b2cbb2543e"},"outputs":[{"output_type":"stream","name":"stdout","text":["14.749051038424174\n","(91844, 300)\n","(22961, 300)\n","(91844, 300)\n","(22961, 300)\n"]}],"source":["# 1°) Finalmente crearemos las matrices de titulares para prueba y entrenamiento de nuestras 2 arquitecturas Word2Vec y haremos\n","# un print() a sus formas respectivas. (Nos apoyaremos en la función matriz() también desarrollada en el curso de\n","# introducción a Word2Vec) --->\n","\n","t0 = time()# Para calcular lo que tarda la Funcion en ejecutar.\n","\n","def matriz(textos,modelo): # Funcion recibe texto y modelo.\n","  x = len(textos) # En X la Longitud de texto que son los Titulares.\n","  y = 300 # Estamos trabajando con un vector denso de 300 posiciones.\n","  matriz = np.zeros((x,y)) # Creamos la Matriz\n","  for i in range(x):\n","    palabras = tokenizador(textos.iloc[i]) # En palabras vamos a ejecutar la Funcion TOKENIZADOR en la cual van a entrar nuestros textos\n","    # y va a localizar cada Subindice[i] que se encuentra dentro de X.\n","    matriz[i] = combinacion_vectores_por_suma(palabras, modelo) # En la posicion de nuestra Matriz en cada Iteracion va a ejecutar la Funcion COMBINACION...\n","    # que recibe a su vez las palabras y el modelo.\n","  return matriz\n","\n","# Ahora cramos la Matriz Entrenamiento y Test para el modelo de CBOW y SKIP-GRAM:\n","matriz_train_cbow = matriz(noticias_train.titulo, modelo_cbow)\n","matriz_test_cbow = matriz(noticias_test.titulo, modelo_cbow)\n","\n","matriz_train_sg = matriz(noticias_train.titulo, modelo_sg)\n","matriz_test_sg = matriz(noticias_test.titulo, modelo_sg)\n","\n","tf = time()-t0\n","\n","print(tf/60)\n","print(matriz_train_cbow.shape)\n","print(matriz_test_cbow.shape)\n","print(matriz_train_sg.shape)\n","print(matriz_test_sg.shape)\n","\n","# Demoro 17 minutos par vectorizar los titulares de los 2 modelos."]},{"cell_type":"markdown","source":["**Explicacion**\n","\n","***¿Entonces qué va a hacer nuestra función matriz?***  Ella va a recibir como parámetros nuestros titulares, nuestros textos, y va a recibir también como parámetro el modelo. Entonces recordemos que vamos a entrenar los dos modelos, tanto el modelo cbow como el modelo Skip Gram.\n","\n","Entonces ella se crea según sus dimensiones que vas hacer el valor de X va a ser la cantidad de filas o la cantidad de titulares que hay en nuestra matriz y el valor de y van a ser las 300 posiciones de nuestro vector denso. Entonces para cada uno de estos titulares entonces serían una variable llamada palabras va aplicar la función tokenizador que es la función que ella crea nuestro objeto nlp.\n","\n","Nosotros lo que hicimos aquí fue eliminar algunos de los pasos o deshabilitar alguno de los pasos como el parser, el tagger, el ner y el textcat, entonces ella va a eliminar todos estos pasos para crear el objeto nlp y sobre el objeto nlp se aplican todas las transformaciones que mencionamos previamente.\n","\n","Después entonces hace la combinación de vectores por suma a todas nuestras palabras utilizando también el modelo, que aquí es donde viene ya utilizar bien sea CBOW o Skip Gram y nos devuelve entonces una matriz. Esto se va a demorar bastante tiempo. Inclusive podemos también usar el parámetro time como lo usamos anteriormente.\n","\n","Veamos al momento de optimizar el tratamiento de los datos, nosotros utilizamos time. Vamos a copiar esta función time, vamos a traerla a vectorizando los titulares y vamos a pegarla. Como ya time estaba importado, no necesitamos importarlo de nuevo. Simplemente vamos a instanciar nuevamente el tiempo.\n","\n","Entonces de esta forma nosotros vamos ya a tomar nuestro horario en este momento. Al final simplemente nos va a imprimir cuánto se demoró. Entonces vamos a cerrar aquí y vamos a ejecutar esta celda y vamos a dejarla rodar."],"metadata":{"id":"RhWmOLvhCziG"}},{"cell_type":"markdown","source":["**Definicion:**\n","\n","***La Matriz de vectores***\n","\n","Hemos explorado y desarrollado funciones auxiliares para utilizar en la creación del clasificador. Entre las funciones desarrolladas, tenemos:\n","\n","* La responsable por el tratamiento y la tokenización de los textos.\n","\n","* La que combina los vectores de cada palabra y crea una representación vectorial para toda la frase.\n","\n","* La función que crea la representación vectorial de toda la base de datos.\n","\n","***Sobre la función que crea la representación vectorial de toda la base de datos:***\n","\n","* Primero se crea la matriz con las dimensiones especificadas para sus filas (x) y columnas (y), después se itera en el rango de las filas, se aplica la función tokenizador() a los textos y se almacena en una variable llamada palabra y esta se convierte en uno de los parámetros que recibe la función combinacion_vectores_por_suma() para finalmente devolver la matriz de vectores.\n","\n"],"metadata":{"id":"8zPBzooy8nFv"}},{"cell_type":"markdown","source":["#**<font color=red>ENTRENAMIENTO E INTEGRACION DEL CLASIFICADOR</font>**\n","\n","---"],"metadata":{"id":"CVt3GJqpGbky"}},{"cell_type":"markdown","metadata":{"id":"fQgvCxlqcYvz"},"source":["##**CLASIFICANDO LOS TITULARES**\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d03ZMD_CyL0B"},"outputs":[],"source":["# 1°) Importamos nuestra REGRESION LOGISTICA Y CLASSIFICATION_REPORT --->\n","\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import classification_report"]},{"cell_type":"code","source":["# 2°) Definimos la Funion CLASIFICADOR --->\n","\n","def clasificador(modelo, X_train, y_train, X_test, y_test):\n","#En la funcion Instanciamos Regression Logistic:\n","  RL = LogisticRegression(max_iter=1500, n_jobs=-1)#N_jobs=maxima capacidad de procesamiento que ofrece google colab.\n","  RL.fit(X_train, y_train) # Entrenamos el modelo\n","  categorias = RL.predict(X_test) # Creamos variable para que almacene el Predict conforme a la matriz de Prueba.\n","  resultados = classification_report(y_test, categorias) # Creamos variable, instanciamos el reporte que recibe Prueba y categoria.\n","  print(resultados)\n","  return RL"],"metadata":{"id":"tf0vmm5dGgKq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 3°) Instanciamos nuestro clasificador --->\n","\n","# Resultados generados antes de corregir la funcion tokenizador(minusculas):\n","t0 = time()\n","RL_cbow = clasificador(modelo_cbow, matriz_train_cbow, noticias_train.categoria, matriz_test_cbow, noticias_test.categoria)\n","tf = time()-t0\n","print(tf/60)\n","\n","# Resultado 54%"],"metadata":{"id":"BXFqa4WEGgNL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1721429198827,"user_tz":180,"elapsed":136157,"user":{"displayName":"mariana ibarra","userId":"08710446199790177814"}},"outputId":"d863051f-c011-476f-cbf9-647eadce4caf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["                      precision    recall  f1-score   support\n","\n","ciencia y tecnologia       0.61      0.55      0.58      2864\n","            deportes       0.70      0.75      0.72      4277\n","            economia       0.60      0.69      0.64      5461\n","     entretenimiento       0.62      0.59      0.61      3580\n","               mundo       0.61      0.61      0.61      2838\n","            politica       0.61      0.51      0.56      3941\n","\n","            accuracy                           0.63     22961\n","           macro avg       0.62      0.62      0.62     22961\n","        weighted avg       0.63      0.63      0.62     22961\n","\n","2.2637718518575034\n"]}]},{"cell_type":"markdown","source":["**RTA:**\n","\n","El resultado es de una EXACTITUD(accuracy) del 0.54(54%), resultado MALO!, esto se debe a que cuando cargamos SPACY desabilitamos algunas funcionalidades para que sea mas rapida la ejecucion, pero hay algo que NO hicimos a nuestro preprocesamiento y que lo habiamos hecho anteriormente que fue colocar todo en MINUSCULA en la funcion TOKENIZADOR, entonces debemos agregarlo:\n","\n","***token_valido.append(token.text) ---> token_valido.append(token.text.lower())***\n","\n","Y como la funcion TOKENIZADOR se usa en la funcion MATRIZ debemos volver a ejecutar ambas funciones nuevamente, para luego volver a ejecutar la funncion Clasificador y comparar los resultados luego de agregar minusculas."],"metadata":{"id":"TDMdN4p8FV9G"}},{"cell_type":"code","source":["# Volvemos a INSTANCIAR Regresion Logistica --->\n","\n","t0 = time()\n","RL_cbow = clasificador(modelo_cbow, matriz_train_cbow, noticias_train.categoria, matriz_test_cbow, noticias_test.categoria)\n","tf = time()-t0\n","print(tf/60)\n","\n","# Podemos ver que nos devuelve una EXACTITUD(accuracy) del 63 % , el hecho de haber colocado todo en MINUSCULA si era parte\n","# del preprocesamiento necesario para generalizar mejor!"],"metadata":{"id":"79WX1qWVGgQG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1721429394978,"user_tz":180,"elapsed":121507,"user":{"displayName":"mariana ibarra","userId":"08710446199790177814"}},"outputId":"56cb361f-d5a9-4a0f-83e2-cdaa902c2fc4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["                      precision    recall  f1-score   support\n","\n","ciencia y tecnologia       0.61      0.55      0.58      2864\n","            deportes       0.70      0.75      0.72      4277\n","            economia       0.60      0.69      0.64      5461\n","     entretenimiento       0.62      0.59      0.61      3580\n","               mundo       0.61      0.61      0.61      2838\n","            politica       0.61      0.51      0.56      3941\n","\n","            accuracy                           0.63     22961\n","           macro avg       0.62      0.62      0.62     22961\n","        weighted avg       0.63      0.63      0.62     22961\n","\n","2.019532104333242\n"]}]},{"cell_type":"markdown","metadata":{"id":"gvWhQb_Dcd11"},"source":["##**COMPARANDO LOS MODELOS**"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":48624,"status":"ok","timestamp":1721430133028,"user":{"displayName":"mariana ibarra","userId":"08710446199790177814"},"user_tz":180},"id":"n63vwy5NyH2U","colab":{"base_uri":"https://localhost:8080/"},"outputId":"709a581a-b0bd-493a-e9e5-f2148ef0a0bc"},"outputs":[{"output_type":"stream","name":"stdout","text":["                      precision    recall  f1-score   support\n","\n","ciencia y tecnologia       0.64      0.56      0.60      2864\n","            deportes       0.72      0.76      0.74      4277\n","            economia       0.62      0.71      0.66      5461\n","     entretenimiento       0.64      0.61      0.62      3580\n","               mundo       0.61      0.63      0.62      2838\n","            politica       0.63      0.53      0.57      3941\n","\n","            accuracy                           0.64     22961\n","           macro avg       0.64      0.63      0.64     22961\n","        weighted avg       0.64      0.64      0.64     22961\n","\n","0.8049120942751566\n"]}],"source":["# 1°) Cambiamos el codigo de CBOW a SG --->\n","\n","t0 = time()\n","RL_sg = clasificador(modelo_sg, matriz_train_sg, noticias_train.categoria, matriz_test_sg, noticias_test.categoria)\n","tf = time()-t0\n","print(tf/60)\n","\n","# Obtuvimos una EXACTITUD del 64 %, si comparams con CBOW que obtuvo el 63 %, pero vemos que se demoro mucho menos, pero recordemos\n","# que en SKIP-GRAM tomamos la palabra para decifrar el contexto, tambien tomamos una ventana mayor que nos ayudo mas a que el modelo\n","# pueda convergir mejor."]},{"cell_type":"markdown","source":["***CONCLUSION:***\n","\n","***¿Entonces qué podemos concluir de lo que hicimos?*** Porque en el curso anterior si nosotros venimos al curso anterior, a los resultados del curso anterior vean ustedes que con el modelo que cargamos de Word2Vec, que era un modelo más holístico, un modelo entrenado con billones de palabras, nos dio un resultado inclusive digamos menos acertado que el resultado que obtuvimos ahorita con el modelo que entrenamos de Skip Gram con muchísimos menos datos.\n","\n","¿Esto porque será? Entonces nosotros entrenamos nuestro modelo directamente considerando el contexto en el cual estábamos trabajando que era un contexto de noticias, de modo que se facilitó mucho esta clasificación porque trabajamos con unos datos que ya tenían de una cierta forma todo el contenido que facilitaría la clasificación.\n","\n","Entonces pues realmente el resultado sí es mejor utilizando directamente nuestro propio WordEmbedding, creado a través pues desde todos los códigos que ejecutamos a lo largo de nuestro entrenamiento. Otro aspecto muy importante es que el modelo que utilizamos en el curso anterior, él tomaba como referencia muchas bases de datos, Wikipedia, noticias, artículos, documentales, entrevistas, de todo.\n","Nosotros aquí fuimos más enfocados en el contenido que nos llamaba la atención o en el contenido de nuestro proyecto que era las noticias directamente. Entonces todo ello facilitó trabajar de una forma más específica porque cuando creamos nuestro propio modelo de Word Embedding, entonces nosotros podemos trabajar de una manera más específica de acuerdo con nuestras necesidades y así nuestros resultados van a ser mucho mejores.\n","\n","Ahora con nuestros modelos listos ya con lo que hemos venido hablando debemos proceder entonces a hacer un dump de nuestros modelos para poderlo utilizar por ejemplo de una API que sea útil a través de una aplicación web. Entonces para ello en una celda nueva de código vamos a hacer importa pickle, y ahora vamos a crear nuestros archivos pickle."],"metadata":{"id":"-3eLazxqOo-o"}},{"cell_type":"code","source":["# 2°) Ahora con nuestro modelos listos vamos a hacer un DUMP para poder utilizarlos en una API, en una aplicacion web --->\n","\n","import pickle\n","\n","with open(\"lr_cbow.pkl\", \"wb\") as f:\n","  pickle.dump(RL_cbow, f)\n","\n","with open(\"lr_sg.pkl\", \"wb\") as f:\n","  pickle.dump(RL_sg, f)"],"metadata":{"id":"J2o0qplNILEa"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}