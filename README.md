#  Word2Vec: Entrenamiento de Word Embedding Propio

[![Python](https://img.shields.io/badge/Python-3670A0?style=flat&logo=python&logoColor=ffdd54)](https://www.python.org/)
[![Pandas](https://img.shields.io/badge/Pandas-150458?style=flat&logo=pandas&logoColor=white)](https://pandas.pydata.org/)
[![spaCy](https://img.shields.io/badge/spaCy-09A3D5?style=flat&logo=spacy&logoColor=white)](https://spacy.io/)
[![Gensim](https://img.shields.io/badge/Gensim-5A7079?style=flat&logo=python&logoColor=white)](https://radimrehurek.com/gensim/models/word2vec.html)

Este proyecto se centra en el desarrollo de representaciones de **Word Embedding** propias mediante el entrenamiento del modelo **Word2Vec** sobre un *dataset* espec铆fico de noticias. El objetivo final es utilizar estas representaciones vectoriales para construir un modelo de clasificaci贸n que pueda ser desplegado en una API.

---

##  Objetivos Principales

El flujo de trabajo cubre el proceso completo desde la obtenci贸n de datos hasta la puesta en producci贸n del modelo:

1.  **Obtenci贸n de Noticias:** Preparar el *dataset* para el modelado.
2.  **Vectorizaci贸n de Titulares:** Crear las representaciones de Word Embedding a partir de los titulares.
3.  **Clasificaci贸n:** Entrenar un modelo para clasificar las noticias por categor铆a.
4.  **Producci贸n:** Colocar el modelo en producci贸n dentro de una API.

---

##  Fases y Metodolog铆a

### 1. Preparaci贸n de Datos
* **Datasets:** Se importan los archivos `noticias_entrenamiento.csv` (91844 registros) y `noticias_prueba.csv` (22961 registros).
* **Variables:** La variable independiente es el **`titulo`** y la variable dependiente (Target) es la **`categoria`**.

### 2. Entrenamiento de Word2Vec
* El n煤cleo del proyecto es la implementaci贸n de **Word2Vec** para modelar representaciones vectoriales del vocabulario de noticias.
* Se utiliza la librer铆a **Gensim** para la implementaci贸n del modelo.
* El desarrollo incluye la **exploraci贸n de spaCy** para el pre-procesamiento del texto.
* El entrenamiento se centra en generar un vocabulario y modelos de *embedding* altamente espec铆ficos para el contenido de las noticias.

### 3. Resultados y Despliegue
* **Modelos Resultantes:** El proceso genera y entrena clasificadores para las arquitecturas **CBOW** (Continuos Bag-of-Words) y **Skip-gram** (ej. `RL_cbow` y `RL_sg`).
* **Serializaci贸n:** Los modelos entrenados se guardan usando la librer铆a **`pickle`** (`lr_cbow.pkl` y `lr_sg.pkl`) para ser utilizados posteriormente en una API o aplicaci贸n web.

---

##  Librer铆as y Recursos Clave

| Librer铆a/Recurso | Uso principal |
| :--- | :--- |
| **Pandas** | Importaci贸n y manipulaci贸n de los datasets de entrenamiento y prueba (`.csv`). |
| **Gensim** | Implementaci贸n y modelado de las arquitecturas **Word2Vec**. |
| **spaCy** | Herramienta de NLP explorada para la construcci贸n del vocabulario y procesamiento. |
| **`logging`** | Configuraci贸n b谩sica de mensajes durante el proceso de construcci贸n del vocabulario. |
| **`pickle`** | Herramienta para serializar y guardar los modelos entrenados (e.g., `lr_cbow.pkl`) para despliegue. |

---

##  Conclusi贸n

Crear un modelo propio de Word Embedding a partir del contenido del proyecto (noticias) es m谩s espec铆fico y produce mejores resultados para las necesidades del proyecto que usar un modelo pre-entrenado general. Los modelos finales est谩n listos para ser utilizados por una API para la clasificaci贸n en tiempo real.
